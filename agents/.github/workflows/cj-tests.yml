name: CJ Agent Behavior Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run tests daily at 9 AM UTC to catch regressions
    - cron: '0 9 * * *'

jobs:
  test-cj-behavior:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Create virtual environment
      run: |
        python -m venv venv
        echo "Virtual environment created"

    - name: Install dependencies
      run: |
        source venv/bin/activate
        pip install --upgrade pip
        pip install -r requirements-dev.txt
        echo "Dependencies installed"

    - name: Start CJ test server
      run: |
        source venv/bin/activate
        python cj_test_server.py &
        echo "Test server started in background"
        # Wait for server to be ready
        sleep 10

    - name: Verify test server health
      run: |
        timeout 30 bash -c 'until curl -f http://localhost:5001/health; do sleep 2; done'
        echo "Test server is healthy"

    - name: Run CJ behavior tests (Mock Evaluator)
      run: |
        source venv/bin/activate
        python run_cj_tests.py --mock-evaluator --output-format json --output-file test_results_mock.json
        echo "Mock evaluation tests completed"

    - name: Run CJ behavior tests (GPT-4 Evaluator)
      if: github.event_name == 'schedule' || github.event_name == 'push'
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: |
        source venv/bin/activate
        python run_cj_tests.py --output-format json --output-file test_results_gpt4.json --sequential
        echo "GPT-4 evaluation tests completed"

    - name: Generate test reports
      if: always()
      run: |
        source venv/bin/activate
        # Generate markdown report for GitHub
        python run_cj_tests.py --mock-evaluator --output-format markdown --output-file test_report.md || true
        # Generate terminal output for logs
        python run_cj_tests.py --mock-evaluator || true

    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: cj-test-results
        path: |
          test_results_*.json
          test_report.md
        retention-days: 30

    - name: Comment PR with test results
      if: github.event_name == 'pull_request' && always()
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');

          try {
            // Read test report if it exists
            let report = '';
            if (fs.existsSync('test_report.md')) {
              report = fs.readFileSync('test_report.md', 'utf8');

              // Truncate if too long for GitHub comment
              if (report.length > 60000) {
                report = report.substring(0, 60000) + '\n\n... (truncated)';
              }
            } else {
              report = 'Test report not available. Check workflow logs for details.';
            }

            const comment = `## ü§ñ CJ Agent Test Results

            ${report}

            <details>
            <summary>View full test artifacts</summary>

            - Download test results JSON from the Actions artifacts
            - Check workflow logs for detailed output
            - Mock evaluator results available for all PRs
            - GPT-4 evaluation runs on main branch pushes and scheduled runs

            </details>`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          } catch (error) {
            console.log('Error posting comment:', error);
          }

    - name: Fail on test failures (GPT-4 only)
      if: github.event_name == 'schedule' || github.event_name == 'push'
      run: |
        source venv/bin/activate
        # Re-run with GPT-4 to get proper exit code
        if [ -n "$OPENAI_API_KEY" ]; then
          python run_cj_tests.py --sequential
        else
          echo "Skipping GPT-4 tests - no API key"
          python run_cj_tests.py --mock-evaluator
        fi
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

  test-framework:
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Create virtual environment
      run: |
        python -m venv venv

    - name: Install dependencies
      run: |
        source venv/bin/activate
        pip install --upgrade pip
        pip install -r requirements-dev.txt

    - name: Run unit tests
      run: |
        source venv/bin/activate
        pytest tests/ -v || echo "No unit tests found - this is expected for now"

    - name: Check code quality
      run: |
        source venv/bin/activate
        # Run linting
        ruff check app/ scripts/ run_cj_tests.py || true
        # Check formatting
        black --check app/ scripts/ run_cj_tests.py || true

    - name: Test strict validation
      run: |
        source venv/bin/activate
        python run_cj_tests.py tests/cj_boundaries/ --strict-validation --mock-evaluator
        echo "Strict validation passed"

  security-check:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Run security scan
      run: |
        # Check for secrets in test files
        if grep -r "sk-" tests/ docs/ || grep -r "OPENAI_API_KEY.*=" tests/ docs/; then
          echo "‚ùå Found potential API keys in test files!"
          exit 1
        fi
        echo "‚úÖ No hardcoded secrets found"

    - name: Check test file permissions
      run: |
        # Ensure test files aren't executable
        find tests/ -name "*.yaml" -executable -type f | while read file; do
          echo "‚ùå Test file $file should not be executable"
          exit 1
        done
        echo "‚úÖ Test file permissions are correct"
