# HireCJ Evals Implementation Plan

## North Stars & Principles

### âš ï¸ CRITICAL: Definition of Success

Success is **not** simply building something that captures conversations and runs tests.
Only elegant, complete solutions that fully embody our principles count as success.

* âŒ Shortcuts = **FAILURE**
* âŒ Halfâ€‘measures = **FAILURE**
* âŒ Compatibility shims = **FAILURE**
* âŒ "Good enough" = **FAILURE**

### ðŸŒŸ Guiding Principles

1. **Longâ€‘Term Elegance Over Shortâ€‘Term Hacks** â€“ Build a proper eval framework that prevents prompt regressions
2. **Break It & Fix It Right** â€“ No backwards compatibility with old conversation formats
3. **Simplicity, Simplify, Simplify** â€“ Basic evals should require no code, just YAML + JSONL
4. **Single Source of Truth** â€“ One eval runner, one registry, one format
5. **No Cruft** â€“ Clean, minimal eval definitions without redundancy
6. **Thoughtful Logging & Instrumentation** â€“ Track eval performance and failures with proper visibility
7. **Infrastructure as Code** â€“ Eval infrastructure deployed via Terraform
8. **Answer Before You Code** â€“ Design the eval framework properly before implementation

---

## Do's âœ… and Do Not's âŒ

### Do's

* Capture complete conversation context including thinking, tool calls, and grounding
* Build on OpenAI's eval patterns but tailored for CrewAI architecture
* Support parallel execution for fast regression testing
* Make evals accessible to non-technical team members
* Track metrics on latency, tokens, and accuracy
* Export conversations in multiple formats (JSON, JSONL, reports)
* Store conversations in a structured file system for version control and easy access

### Do Not's

* ðŸš« Create multiple eval frameworks or runners
* ðŸš« Build backwards compatibility for old conversation formats
* ðŸš« Over-engineer with features we don't need yet
* ðŸš« Create complex eval DSLs when YAML suffices
* ðŸš« Mix eval code with production agent code
* ðŸš« Store sensitive customer data in eval datasets
* ðŸš« Use browser local storage for conversation persistence

---

## ðŸš§ Implementation Status Banner

> **ðŸš€ CURRENT PHASE:** *Planning* ðŸ”µ Complete
> **ðŸ”œ NEXT STEPS:** *Milestone 1 â€“ Conversation Capture Infrastructure*

## Executive Summary

Build an elegant evaluation system for HireCJ that captures agent conversations from the editor and uses them to systematically test and improve prompts. The system will catch prompt regressions, ensure consistent agent behavior across workflows, and enable rapid iteration on agent capabilities. Inspired by OpenAI's evals framework but tailored to CrewAI's multi-agent architecture, with a file-based storage system modeled after third_party/evals.

---

## Architecture Snapshot â€“ Before vs. After

### Onâ€‘Disk Layout

|                 | **Before**        | **After**           |                   |
| --------------- | ----------------- | ------------------- | ----------------- |
| `editor/src/`   | Basic playground  | `editor/src/evals/` | Eval framework, capture, runner |
| `backend/app/`  | Agent endpoints   | `backend/app/api/v1/evals/` | Eval execution API |
| N/A             | No eval data      | `hirecj_evals/` | Structured eval data directory |
| N/A             | No conversations  | `hirecj_evals/conversations/` | Date-organized JSON captures |
| N/A             | No test datasets  | `hirecj_evals/datasets/` | JSONL test cases |
| N/A             | No eval configs   | `hirecj_evals/registry/` | YAML eval definitions |

### Conceptual / Object Hierarchies

|                | **Before** | **After**      |       |
| -------------- | ---------- | -------------- | ----- |
| Message Storage | In-memory only | File-based `ConversationCapture` | Persistent JSON files |
| Testing | Manual playground | `EvalRunner` with file datasets | Automated regression testing |
| Prompts | Static files | Version-tracked with evals | A/B testing capability |
| Data Organization | None | Date/source-based directories | Easy batch processing |

---

## Overview
An elegant evaluation system for HireCJ that captures conversations from the editor and uses them to systematically test and improve prompts. Inspired by OpenAI's evals framework, but tailored to our CrewAI agent architecture and multi-step reasoning patterns.

## File System Structure

Following the pattern established by `third_party/evals`, we'll create a structured directory system:

```
hirecj_evals/
â”œâ”€â”€ conversations/           # Raw captured conversations (JSON)
â”‚   â”œâ”€â”€ playground/         # From playground testing
â”‚   â”‚   â””â”€â”€ 2024-06-20/    # Date-based organization
â”‚   â”‚       â”œâ”€â”€ conv_abc123.json
â”‚   â”‚       â””â”€â”€ conv_def456.json
â”‚   â”œâ”€â”€ production/         # From production use (sanitized)
â”‚   â”‚   â””â”€â”€ 2024-06-20/
â”‚   â””â”€â”€ synthetic/          # Generated test conversations
â”‚       â””â”€â”€ edge_cases/
â”œâ”€â”€ datasets/               # JSONL eval datasets
â”‚   â”œâ”€â”€ golden/            # Manually curated test cases
â”‚   â”‚   â”œâ”€â”€ tool_selection.jsonl
â”‚   â”‚   â”œâ”€â”€ grounding_accuracy.jsonl
â”‚   â”‚   â””â”€â”€ workflow_compliance.jsonl
â”‚   â”œâ”€â”€ generated/         # Auto-generated from conversations
â”‚   â”‚   â””â”€â”€ 2024-06-20/
â”‚   â””â”€â”€ regression/        # Specific regression tests
â”‚       â””â”€â”€ issue_123_tool_fix.jsonl
â”œâ”€â”€ registry/              # YAML eval definitions
â”‚   â”œâ”€â”€ base.yaml         # Base eval configurations
â”‚   â”œâ”€â”€ cj_responses.yaml
â”‚   â”œâ”€â”€ tool_usage.yaml
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ad_hoc_support.yaml
â”‚       â””â”€â”€ conversation_flow.yaml
â”œâ”€â”€ results/               # Eval run results
â”‚   â”œâ”€â”€ runs/             # Individual run data
â”‚   â”‚   â””â”€â”€ 2024-06-20/
â”‚   â”‚       â””â”€â”€ run_xyz789/
â”‚   â””â”€â”€ reports/          # Aggregated reports
â”‚       â””â”€â”€ weekly/
â””â”€â”€ README.md             # Documentation
```

## Core Architecture

### 1. Conversation Capture Layer
**Purpose**: Capture complete conversation context for reproducible evaluation

```typescript
interface ConversationCapture {
  // Unique identifier
  id: string;
  timestamp: string;
  
  // Full execution context
  context: {
    workflow: WorkflowConfig;
    persona: Persona;
    scenario: Scenario;
    trustLevel: number;
    model: string;
    temperature: number;
  };
  
  // System prompts at time of execution
  prompts: {
    cj_prompt: string;
    workflow_prompt: string;
    tool_definitions: ToolDefinition[];
  };
  
  // Complete conversation history
  messages: Array<{
    turn: number;
    user_input: string;
    
    // Full agent processing chain
    agent_processing: {
      thinking: string;
      intermediate_responses: string[];
      tool_calls: ToolCall[];
      grounding_queries: GroundingQuery[];
      final_response: string;
    };
    
    // Performance metrics
    metrics: {
      latency_ms: number;
      tokens: {
        prompt: number;
        completion: number;
        thinking: number;
      };
    };
  }>;
}
```

### 2. Eval Registry System
**Pattern**: YAML-based registry for discovering and configuring evaluations

```yaml
# editor/evals/registry/cj_responses.yaml
cj_response_quality:
  id: cj_response_quality.v1
  description: "Evaluates CJ's response quality and tool usage"
  
  # Eval class to use
  class: evals.cj.ResponseQuality
  
  # Default arguments
  args:
    metrics:
      - response_helpfulness
      - tool_selection_accuracy
      - grounding_relevance
    threshold: 0.85

cj_response_quality.conversation_flow:
  parent: cj_response_quality
  args:
    workflow_filter: "conversation_flow"
    additional_metrics:
      - conversation_coherence
      - context_retention
```

### 3. Eval Data Format
**Pattern**: JSONL format compatible with OpenAI evals, extended for CrewAI

```jsonl
{
  "eval_id": "tool_selection_accuracy",
  "sample_id": "conv_abc123_turn_2",
  
  // Input context
  "input": {
    "messages": [
      {"role": "system", "content": "..."},
      {"role": "user", "content": "test a tool"},
      {"role": "assistant", "content": "I'll help you test..."},
      {"role": "user", "content": "yeah try a diff tool"}
    ],
    "context": {
      "workflow": "ad_hoc_support",
      "available_tools": ["get_shopify_store_counts", "get_orders", ...]
    }
  },
  
  // Expected behavior
  "ideal": {
    "tool_selection": {
      "should_use_tool": true,
      "acceptable_tools": ["get_shopify_store_counts", "get_store_analytics"],
      "unacceptable_tools": ["get_orders"]
    },
    "response_criteria": {
      "must_include": ["different tool", "Shopify"],
      "tone": "helpful",
      "explains_choice": true
    }
  },
  
  // Actual output from conversation
  "actual": {
    "thinking": "The user asked for a different tool...",
    "tool_calls": ["get_shopify_store_counts"],
    "response": "I'll use the Shopify store counts tool..."
  },
  
  // Metadata for analysis
  "metadata": {
    "source_conversation": "playground_abc123",
    "turn": 2,
    "timestamp": "2024-06-19T22:35:00Z"
  }
}
```

### 4. Eval Types Hierarchy

```python
# Base eval types (inspired by OpenAI)
class CJEval(ABC):
    """Base class for all HireCJ evaluations"""
    
    @abstractmethod
    def eval_sample(self, sample: EvalSample) -> EvalResult:
        pass

class MatchEval(CJEval):
    """Exact or fuzzy matching of responses"""
    
class IncludesEval(CJEval):
    """Checks if response includes required elements"""
    
class ModelGradedEval(CJEval):
    """Uses another model to grade the response"""

# HireCJ-specific eval types
class ToolSelectionEval(CJEval):
    """Evaluates if correct tools were selected"""
    
class GroundingAccuracyEval(CJEval):
    """Evaluates grounding query relevance and usage"""
    
class ConversationFlowEval(CJEval):
    """Evaluates multi-turn conversation coherence"""
    
class WorkflowComplianceEval(CJEval):
    """Ensures responses follow workflow rules"""
```

## Progress Summary

### Completed âœ…
- **Phase 1: Conversation Capture Infrastructure** âœ… COMPLETED
  - âœ… Created TypeScript types for conversation capture
  - âœ… Implemented useConversationCapture React hook  
  - âœ… Built conversation capture endpoint in agents service
  - âœ… Implemented file-based storage with date organization
  - âœ… Created proxy endpoint in editor-backend for complete integration
  
- **Phase 2: Eval Framework Core**
  - âœ… Built base evaluation classes (ExactMatch, FuzzyMatch, Includes, ModelGraded)
  - âœ… Created YAML-based registry system with inheritance
  - âœ… Implemented parallel eval runner
  - âœ… Built HireCJ-specific evaluators (ToolSelectionAccuracy, ResponseQuality, etc.)
  - âœ… Created CLI tool for running evaluations
  - âœ… Added colorful number-driven CLI interface
  - âœ… Implemented conversion tool for captured conversations to JSONL format
  - âœ… Created comprehensive test datasets
  - âœ… Added privacy scrubbing utility
  - âœ… Fixed conversation capture to include full agent processing details (thinking, intermediate responses, tool calls, grounding queries)

### In Progress ðŸš§
- Phase 3: Editor Integration - Eval Designer View

### Next Steps ðŸ“‹
- Phase 3: Editor Integration (Eval Designer View, Batch Testing, Results Dashboard)
- Phase 4: Advanced Features (Continuous Evaluation, Smart Test Generation)
- Production model-graded evaluations with GPT-4

## Implementation Phases

### Phase 1: Conversation Capture Infrastructure âœ… COMPLETED
**Goal**: Reliably capture all conversation data to structured file system

1. **Enhanced Message Recording** âœ…
   - Implemented `useConversationCapture` hook in editor frontend
   - Tracks full conversation context including thinking states
   - Captures tool calls and grounding queries
   - "Export for Eval" button added to PlaygroundView

2. **Backend Storage API** âœ… COMPLETED
   
   **Current Architecture:**
   ```
   Editor Frontend â†’ Editor-Backend (proxy) â†’ Agents Service (storage)
         â†“                    â†“                        â†“
   /api/v1/conversations â†’ Forward request â†’ /api/v1/conversations/capture
                                                      â†“
                                            hirecj_evals/conversations/
                                                  playground/
                                                    2024-06-20/
                                                      conv_abc123.json
   ```
   
   **Implementation Status:**
   - âœ… Capture endpoint exists in agents service at `/api/v1/conversations/capture`
   - âœ… File-based storage with date organization implemented
   - âœ… All models and validation in place
   - âœ… Proxy endpoint in editor-backend implemented and connected
   
   **Implementation Details:**
   ```python
   # agents/app/api/routes/conversations.py - IMPLEMENTED
   @router.post("/capture")
   async def capture_conversation(request: CaptureRequest):
       """Capture a conversation for evaluation purposes."""
       # Creates: hirecj_evals/conversations/{source}/{date}/{id}.json
   ```
   
   ```python
   # editor-backend/app/api/routes/conversations.py - IMPLEMENTED
   @router.post("/capture") 
   async def proxy_capture(request: Request):
       """Forward capture requests to agents service."""
       # Proxies to agents service with error handling and logging
   ```

3. **Export & Conversion Tools** âœ…
   - âœ… CLI tool to convert conversations to JSONL eval format (`scripts/convert_conversations.py`)
   - âœ… Batch processing for date ranges
   - âœ… Privacy scrubbing for production data (`scripts/scrub_conversations.py`)
   - âœ… Git integration (.gitignore for sensitive paths)

### Phase 2: Eval Framework Core âœ… COMPLETED
**Goal**: Build the evaluation engine

1. **Registry System**
   ```typescript
   // Eval registry loader
   class EvalRegistry {
     loadEvals(pattern: string = "evals/**/*.yaml") {
       // Discover and parse eval definitions
     }
     
     getEval(evalId: string): EvalConfig {
       // Resolve eval with inheritance
     }
   }
   ```

2. **Eval Runner**
   ```typescript
   class EvalRunner {
     async run(evalId: string, options: RunOptions) {
       const eval = registry.getEval(evalId);
       const samples = await loadSamples(eval.dataset);
       
       // Parallel execution with progress tracking
       const results = await pmap(samples, 
         sample => this.evalSample(eval, sample),
         { concurrency: options.workers }
       );
       
       return this.aggregateResults(results);
     }
   }
   ```

3. **Recording System**
   ```typescript
   // Event-based recording (like OpenAI)
   class EvalRecorder {
     record(event: EvalEvent) {
       // Record to multiple backends
       this.backends.forEach(b => b.record(event));
     }
   }
   ```

### Phase 3: Editor Integration
**Goal**: Seamless eval workflow in the editor

1. **Eval Designer View**
   - Visual interface for creating eval cases
   - Convert conversations to test cases
   - Edit expected outputs
   - Preview eval execution

2. **Batch Testing Interface**
   ```typescript
   // Run evals against multiple prompt versions
   const comparePrompts = async (evalId: string, prompts: PromptVersion[]) => {
     const results = await Promise.all(
       prompts.map(p => runEval(evalId, { promptOverride: p }))
     );
     return generateComparisonReport(results);
   };
   ```

3. **Results Dashboard**
   - Real-time eval progress
   - Detailed failure analysis
   - Prompt version comparison
   - Export results for further analysis

### Phase 4: Advanced Features
**Goal**: Production-ready evaluation system

1. **Continuous Evaluation**
   - Webhook triggers on prompt changes
   - Automated regression detection
   - Performance benchmarking

2. **Smart Test Generation**
   ```python
   class TestGenerator:
       def generate_edge_cases(self, conversation: Conversation):
           # Analyze conversation for patterns
           # Generate variations and edge cases
           # Suggest missing test coverage
   ```

3. **Model-Graded Evals**
   - Use GPT-4 to evaluate response quality
   - Custom rubrics for different workflows
   - Human-in-the-loop validation

## File Structure

```
# Root-level eval data directory (following third_party/evals pattern)
hirecj_evals/
â”œâ”€â”€ conversations/             # Raw captured conversations
â”‚   â”œâ”€â”€ playground/           # Playground testing
â”‚   â”œâ”€â”€ production/           # Production (sanitized)
â”‚   â””â”€â”€ synthetic/            # Generated conversations
â”œâ”€â”€ datasets/                 # JSONL eval datasets  
â”‚   â”œâ”€â”€ golden/              # Manually curated
â”‚   â”œâ”€â”€ generated/           # Auto-generated
â”‚   â””â”€â”€ regression/          # Regression tests
â”œâ”€â”€ registry/                # YAML eval definitions
â”‚   â”œâ”€â”€ base.yaml
â”‚   â”œâ”€â”€ cj_responses.yaml
â”‚   â”œâ”€â”€ tool_usage.yaml
â”‚   â””â”€â”€ workflows/
â”œâ”€â”€ results/                 # Eval run results
â”‚   â”œâ”€â”€ runs/               # Individual runs
â”‚   â””â”€â”€ reports/            # Aggregated reports
â””â”€â”€ README.md

# Application code
editor/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ evals/
â”‚   â”‚   â”œâ”€â”€ core/              # Eval framework
â”‚   â”‚   â”‚   â”œâ”€â”€ registry.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ runner.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ recorder.ts
â”‚   â”‚   â”‚   â””â”€â”€ types.ts
â”‚   â”‚   â””â”€â”€ evaluators/        # Eval implementations
â”‚   â”‚       â”œâ”€â”€ base.ts
â”‚   â”‚       â”œâ”€â”€ toolSelection.ts
â”‚   â”‚       â”œâ”€â”€ grounding.ts
â”‚   â”‚       â””â”€â”€ workflow.ts
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â””â”€â”€ evals/
â”‚   â”‚       â”œâ”€â”€ EvalDesigner.tsx
â”‚   â”‚       â”œâ”€â”€ EvalRunner.tsx
â”‚   â”‚       â””â”€â”€ ResultsViewer.tsx
â”‚   â””â”€â”€ hooks/
â”‚       â”œâ”€â”€ useEvals.ts
â”‚       â””â”€â”€ useConversationCapture.ts

backend/
â”œâ”€â”€ app/
â”‚   â””â”€â”€ api/
â”‚       â””â”€â”€ v1/
â”‚           â”œâ”€â”€ conversations/  # Conversation capture
â”‚           â”‚   â”œâ”€â”€ __init__.py
â”‚           â”‚   â”œâ”€â”€ capture.py
â”‚           â”‚   â””â”€â”€ converter.py
â”‚           â””â”€â”€ evals/         # Eval execution API
â”‚               â”œâ”€â”€ __init__.py
â”‚               â”œâ”€â”€ runner.py
â”‚               â””â”€â”€ storage.py

# CLI tools
scripts/
â”œâ”€â”€ convert_conversations.py   # Convert JSON to JSONL
â”œâ”€â”€ run_evals.py              # CLI eval runner
â””â”€â”€ generate_report.py        # Create eval reports
```

## Key Design Principles

1. **Simplicity First**: Basic evals should require no code, just YAML + JSONL
2. **Extensibility**: Easy to add custom eval types for HireCJ-specific needs
3. **Reproducibility**: Every eval run is deterministic and traceable
4. **Performance**: Parallel execution, caching, and incremental evaluation
5. **Integration**: Works seamlessly with existing editor and backend

## Success Metrics

1. **Coverage**: 80%+ of production conversations can be converted to eval cases
2. **Speed**: Eval suite runs in <5 minutes for regression testing
3. **Accuracy**: Catch 95%+ of prompt regressions before deployment
4. **Usability**: Non-technical team members can create and run evals

## Next Steps

1. Start with Phase 1: Implement robust conversation capture
2. Build minimal eval runner for tool selection accuracy
3. Create first batch of golden test cases from real conversations
4. Iterate based on team feedback

This implementation plan provides a solid foundation that captures the elegance of OpenAI's eval framework while being specifically tailored to HireCJ's unique architecture and needs.